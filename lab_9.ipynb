{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXk9_1a-y95"
      },
      "source": [
        "# Biblioteki Pythona w analizie danych\n",
        "### Tomasz Rodak\n",
        "\n",
        "Lab 9\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzvX0iMS-y96"
      },
      "source": [
        "## Sieci konwolucyjne - podstawy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWW1Wl5m-y96"
      },
      "source": [
        "### 1.\n",
        "\n",
        "Oblicz $\\mathbf{I} \\ast \\mathbf{K}$ dla:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{I} = \\begin{bmatrix}\n",
        "2 & 5 & -3 & 0 \\\\\n",
        "0 & 6 & 0 & -4 \\\\\n",
        "-1 & -3 & 0 & 2 \\\\\n",
        "5 & 0 & 0 & 3\n",
        "\\end{bmatrix}\n",
        "\\text{ oraz } \\mathbf{K} = \\begin{bmatrix}\n",
        "-2 & 0 \\\\\n",
        "4 & 6\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXRacTO-y97"
      },
      "source": [
        "### 2.\n",
        "\n",
        "Rozważmy fragment sieci konwolucyjnej o architekturze:\n",
        "\n",
        "**Wejście:** Obraz $x$ o rozmiarze $1 \\times 224 \\times 224$\n",
        "\n",
        "| Nr | Warstwa            | Parametry                                                                      |\n",
        "| -- | ------------------ | ------------------------------------------------------------------------------ |\n",
        "| 1  | $\\mathrm{Conv}$ C1 | filtr $5\\times5$, stride=1, padding=0 i padding=2 (warianty), 6 filtrów, ReLU  |\n",
        "| 2  | $\\mathrm{Conv}$ C2 | filtr $3\\times3$, stride=1, padding=0 i padding=1 (warianty), 16 filtrów, ReLU |\n",
        "\n",
        "\n",
        "\n",
        "Oblicz:\n",
        "1. Rozmiar wyjścia z warstwy C1.\n",
        "2. Rozmiar wyjścia z warstwy C2.\n",
        "3. Rozmiar okna (*receptive field*) na obrazie $x$ \"widzianego\" przez filtr konwolucyjny z warstwy C2.\n",
        "4. Dokładną liczbę parametrów (zarówno wag, jak i biasów) w warstwach C1 oraz C2.\n",
        "\n",
        "Zbuduj ten fragment sieci konwolucyjnej w PyTorch i zweryfikuj poprawność obliczeń.\n",
        "\n",
        "Rozwiąż to samo zadanie dla architektury:\n",
        "\n",
        "**Wejście:** Obraz $x$ o rozmiarze $1 \\times 224 \\times 224$\n",
        "| Nr | Warstwa            | Parametry                                                                      |\n",
        "| -- | ------------------ | ------------------------------------------------------------------------------ |\n",
        "| 1  | $\\mathrm{Conv}$ C1 | filtr $5\\times5$, stride=2, padding=0 i padding=2 (warianty), 6 filtrów, ReLU  |\n",
        "| 2  | $\\mathrm{Conv}$ C2 | filtr $3\\times3$, stride=2, padding=0 i padding=1 (warianty), 16 filtrów, ReLU |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.relu(self.conv2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "j2kqaptDIYmc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 1, 224, 224)\n",
        "model = CNN()\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683UfqW9KCe_",
        "outputId": "07a94eea-9170-4d24-f3b5-67ccda69c77c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 218, 218])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "  print(f\"{name}: {p.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJddnxpKws8",
        "outputId": "571e00e6-1115-498a-dc0c-e0fbd374e065"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight: torch.Size([6, 1, 5, 5])\n",
            "conv1.bias: torch.Size([6])\n",
            "conv2.weight: torch.Size([16, 6, 3, 3])\n",
            "conv2.bias: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKxK4_E7L8vf",
        "outputId": "b9d773b2-5b68-4ee7-bc60-65f9f16a28f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchinfo\n",
        "torchinfo.summary(model, input_size=(1, 1, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d91GeQ_FL_xd",
        "outputId": "87b51009-a579-42d2-e399-b2949241a692"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [1, 16, 218, 218]         --\n",
              "├─Conv2d: 1-1                            [1, 6, 220, 220]          156\n",
              "├─ReLU: 1-2                              [1, 6, 220, 220]          --\n",
              "├─Conv2d: 1-3                            [1, 16, 218, 218]         880\n",
              "├─ReLU: 1-4                              [1, 16, 218, 218]         --\n",
              "==========================================================================================\n",
              "Total params: 1,036\n",
              "Trainable params: 1,036\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 49.37\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 8.41\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 8.61\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xzaTKGJ-y98"
      },
      "source": [
        "### 3.\n",
        "\n",
        "\n",
        "Dany jest fragment sieci konwolucyjnej o architekturze:\n",
        "\n",
        "**Wejście:** Obraz $x$ o rozmiarze $1 \\times 224 \\times 224$.\n",
        "\n",
        "| Nr | Warstwa            | Parametry                                                                      |\n",
        "| -- | ------------------ | ------------------------------------------------------------------------------ |\n",
        "| 1  | $\\mathrm{Conv}$ C1 | filtr $5\\times5$, stride=1, padding=0, 6 filtrów, ReLU                        |\n",
        "| 2  | MaxPool P1         | okno $2\\times2$, stride=2                                                      |\n",
        "| 3  | $\\mathrm{Conv}$ C2 | filtr $3\\times3$, stride=1, padding=0, 16 filtrów, ReLU                       |\n",
        "| 4  | MaxPool P2         | okno $2\\times2$, stride=2                                                      |\n",
        "\n",
        "\n",
        "\n",
        "Oblicz:\n",
        "1. Rozmiary wyjścia z każdej kolejnej warstwy.\n",
        "2. Rozmiar okna na obrazie $x$ \"widzianego\" przez okno poolingu z warstwy P1.\n",
        "3. Rozmiar okna na obrazie $x$ \"widzianego\" przez filtr konwolucyjny z warstwy C2.\n",
        "4. Rozmiar okna na obrazie $x$ \"widzianego\" przez okno pooling z warstwy P2.\n",
        "5. Dokładną liczbę parametrów w sieci.\n",
        "\n",
        "Zbuduj ten fragment sieci konwolucyjnej w PyTorch i zweryfikuj poprawność obliczeń."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slqnGAr6-y98"
      },
      "source": [
        "### 4.\n",
        "\n",
        "Zaproponuj własną architekturę sieci CNN do klasyfikacji obrazów CIFAR-10. Są to obrazy kolorowe o rozmiarze $32 \\times 32$ pikseli i 10 klasach.\n",
        "\n",
        "Wykonaj testowy trening, aby upewnić się, że dane są poprawnie wczytywane i architektura działa. Dane można wczytać z repozytoriów PyTorch:\n",
        "\n",
        "```python\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "2fgAO-15bJpK",
        "outputId": "54c1c57a-624e-420d-8a25-9821e900e262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 15.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Zaproponuj własną architekturę sieci CNN do klasyfikacji obrazów CIFAR-10. Są to obrazy kolorowe o rozmiarze 32×32 pikseli i 10 klasach.\n",
        "# Wykonaj testowy trening, aby upewnić się, że dane są poprawnie wczytywane i architektura działa. Dane można wczytać z repozytoriów PyTorch:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512) # Adjusted for CIFAR-10 image size\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8) # Adjusted for CIFAR-10 image size\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Perform a test training loop (one epoch)\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "EFhbNsijcxnc",
        "outputId": "16873c64-22d3-4db1-eede-955209372d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 1.603\n",
            "[1,   400] loss: 1.284\n",
            "[1,   600] loss: 1.153\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vyex25w-y98"
      },
      "source": [
        "### 5.\n",
        "\n",
        "Oblicz ręcznie liczbę parametrów w sieci VGG16. Architekturę sieci znajdziesz w [C. Bishop, Deep Learning](https://www.bishopbook.com/), strona 300.\n",
        "\n",
        "Potwierdź poprawność obliczeń w PyTorch. Model z losowymi wagami (szybciej się ładuje) możesz pobrać przy pomocy:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=False)\n",
        "```\n",
        "\n",
        "Zobacz [torchvision.models](https://pytorch.org/vision/stable/models.html#torchvision.models.vgg16)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "vgg16 = models.vgg16(weights=None)"
      ],
      "metadata": {
        "id": "0iZE-ye8bRAj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 224, 224)\n",
        "output = vgg16(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "ONt3abuibXmh",
        "outputId": "5906ae74-e2b2-4e89-90e1-3f3387394f5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name,p in vgg16.named_parameters():\n",
        "  print(f\"{name}: {p.shape}, liczba elementow: {p.numel()}\")"
      ],
      "metadata": {
        "id": "juaQvVW7bhb5",
        "outputId": "b59dbd21-9cf0-4209-d55d-0bf558b47c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.0.weight: torch.Size([64, 3, 3, 3]), liczba elementow: 1728\n",
            "features.0.bias: torch.Size([64]), liczba elementow: 64\n",
            "features.2.weight: torch.Size([64, 64, 3, 3]), liczba elementow: 36864\n",
            "features.2.bias: torch.Size([64]), liczba elementow: 64\n",
            "features.5.weight: torch.Size([128, 64, 3, 3]), liczba elementow: 73728\n",
            "features.5.bias: torch.Size([128]), liczba elementow: 128\n",
            "features.7.weight: torch.Size([128, 128, 3, 3]), liczba elementow: 147456\n",
            "features.7.bias: torch.Size([128]), liczba elementow: 128\n",
            "features.10.weight: torch.Size([256, 128, 3, 3]), liczba elementow: 294912\n",
            "features.10.bias: torch.Size([256]), liczba elementow: 256\n",
            "features.12.weight: torch.Size([256, 256, 3, 3]), liczba elementow: 589824\n",
            "features.12.bias: torch.Size([256]), liczba elementow: 256\n",
            "features.14.weight: torch.Size([256, 256, 3, 3]), liczba elementow: 589824\n",
            "features.14.bias: torch.Size([256]), liczba elementow: 256\n",
            "features.17.weight: torch.Size([512, 256, 3, 3]), liczba elementow: 1179648\n",
            "features.17.bias: torch.Size([512]), liczba elementow: 512\n",
            "features.19.weight: torch.Size([512, 512, 3, 3]), liczba elementow: 2359296\n",
            "features.19.bias: torch.Size([512]), liczba elementow: 512\n",
            "features.21.weight: torch.Size([512, 512, 3, 3]), liczba elementow: 2359296\n",
            "features.21.bias: torch.Size([512]), liczba elementow: 512\n",
            "features.24.weight: torch.Size([512, 512, 3, 3]), liczba elementow: 2359296\n",
            "features.24.bias: torch.Size([512]), liczba elementow: 512\n",
            "features.26.weight: torch.Size([512, 512, 3, 3]), liczba elementow: 2359296\n",
            "features.26.bias: torch.Size([512]), liczba elementow: 512\n",
            "features.28.weight: torch.Size([512, 512, 3, 3]), liczba elementow: 2359296\n",
            "features.28.bias: torch.Size([512]), liczba elementow: 512\n",
            "classifier.0.weight: torch.Size([4096, 25088]), liczba elementow: 102760448\n",
            "classifier.0.bias: torch.Size([4096]), liczba elementow: 4096\n",
            "classifier.3.weight: torch.Size([4096, 4096]), liczba elementow: 16777216\n",
            "classifier.3.bias: torch.Size([4096]), liczba elementow: 4096\n",
            "classifier.6.weight: torch.Size([1000, 4096]), liczba elementow: 4096000\n",
            "classifier.6.bias: torch.Size([1000]), liczba elementow: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDhXg11B-y99"
      },
      "source": [
        "### 6.\n",
        "\n",
        "Zaimplementuj w PyTorch własną sekwencyjną sieć konwolucyjną (bez pretrenowanych wag) do klasyfikacji zbioru Fashion-MNIST. Architektura powinna zawierać:\n",
        "\n",
        "1. Co najmniej 2 warstwy konwolucyjne\n",
        "2. Warstwę `BatchNorm2d` po każdej konwolucji\n",
        "3. Funkcje aktywacji `ReLU`\n",
        "4. Pooling do redukcji wymiarowości\n",
        "5. Klasyfikator w pełni połączony na końcu sieci\n",
        "\n",
        "Zadanie obejmuje:\n",
        "- Implementację modelu\n",
        "- Przygotowanie danych treningowych i testowych\n",
        "- Napisanie funkcji trenującej\n",
        "- Ewaluację dokładności na zbiorze testowym"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}